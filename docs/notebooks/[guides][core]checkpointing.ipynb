{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœ… Checkpointing\n",
    "\n",
    "In this example, saving and restoring a simple model is demonstrated using [orbax](https://orbax.readthedocs.io/en/latest/index.html) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ASEM000/serket --quiet\n",
    "!pip install orbax-checkpoint --quiet\n",
    "!pip install optax --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asem/miniforge3/envs/py311/lib/python3.11/site-packages/orbax/checkpoint/type_handlers.py:100: UserWarning: Serialization for `jax.sharding.SingleDeviceSharding` has not been implemented.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import serket as sk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.tree_util as jtu\n",
    "import orbax.checkpoint as ocp\n",
    "import optax\n",
    "\n",
    "net = sk.Sequential(\n",
    "    sk.nn.Linear(1, 128, key=jr.PRNGKey(0)),\n",
    "    jax.nn.relu,\n",
    "    sk.nn.Linear(128, 1, key=jr.PRNGKey(1)),\n",
    ")\n",
    "\n",
    "# exclude non-parameters\n",
    "net = sk.tree_mask(net)\n",
    "\n",
    "# 1) get flat parameters and the tree structure\n",
    "flat_net, treedef = jtu.tree_flatten(net)\n",
    "\n",
    "# 2) define a checkpointer and save the parameters\n",
    "checkpointer = ocp.PyTreeCheckpointer()\n",
    "checkpointer.save(\"ckpt1\", flat_net)\n",
    "\n",
    "# 3) load the flat parameters\n",
    "flat_net = checkpointer.restore(\"ckpt1\")\n",
    "\n",
    "# 4) reconstruct the tree using the loaded flat parameters and the tree structure\n",
    "loaded_net = jtu.tree_unflatten(treedef, flat_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Managing checkpoints\n",
    "\n",
    "For checkpointed saving, `orbax` offers the ability to define set of options to configure the process.\n",
    "\n",
    "For full guide check [here](https://orbax.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = ocp.CheckpointManager(\n",
    "    directory=\"ckpt2\",\n",
    "    # lets assume we want to save neural network parameters and optimizer state\n",
    "    # then we need to define a checkpointers dict with the keys \"net\" and \"state\"\n",
    "    checkpointers=dict(net=ocp.PyTreeCheckpointer(), state=ocp.PyTreeCheckpointer()),\n",
    "    # save checkpoints every 2 steps and keep the last 3 checkpoints\n",
    "    options=ocp.CheckpointManagerOptions(max_to_keep=3, save_interval_steps=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(net, x, y):\n",
    "    net = sk.tree_unmask(net)\n",
    "    return jnp.mean((jax.vmap(net)(x) - y) ** 2)\n",
    "\n",
    "\n",
    "optim = optax.adam(1e-3)\n",
    "optim_state = optim.init(net)\n",
    "optim_state_treedef = jtu.tree_structure(optim_state)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(net, optim_state: optax.OptState, x: jax.Array, y: jax.Array):\n",
    "    loss, grads = jax.value_and_grad(loss_func)(net, x, y)\n",
    "    updates, optim_state = optim.update(grads, optim_state)\n",
    "    net = optax.apply_updates(net, updates)\n",
    "    return net, optim_state, loss\n",
    "\n",
    "\n",
    "x = jax.random.uniform(jax.random.PRNGKey(0), (100, 1))\n",
    "y = jnp.sin(x) + jax.random.normal(jax.random.PRNGKey(0), (100, 1)) * 0.1\n",
    "\n",
    "# should save step [0, 2, 4, 6, 8], and keep the last 3 checkpoints\n",
    "# namely step [4, 6, 8]\n",
    "\n",
    "for step in range(10):\n",
    "    net, optim_state, loss = train_step(net, optim_state, x, y)\n",
    "    flat_net = jtu.tree_leaves(net)\n",
    "    flat_optim_state = jtu.tree_leaves(optim_state)\n",
    "    # note that we need to save the *flat* parameters and the *flat* optimizer state\n",
    "    manager.save(step, dict(net=flat_net, state=flat_optim_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6, 8]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all the checkpoints\n",
    "manager.all_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint at step 6\n",
    "checkpointers = manager.restore(6)\n",
    "\n",
    "loaded_flat_net = checkpointers[\"net\"]\n",
    "loaded_optim_flat_state = checkpointers[\"state\"]\n",
    "\n",
    "# reconstruct the tree with the loaded parameters\n",
    "loaded_net = jtu.tree_unflatten(treedef, loaded_flat_net)\n",
    "loaded_optim_state = jtu.tree_unflatten(optim_state_treedef, loaded_optim_flat_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

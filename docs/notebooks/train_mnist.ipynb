{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0i-yEycZkF-"
      },
      "source": [
        "# Train `MNIST`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdM9pVTUZkGC"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HL0vQXylZmcw",
        "outputId": "b380b72a-649c-4897-a482-f260f63a6809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.82 requires jax>=0.4.6, which is not installed.\n",
            "chex 0.1.82 requires toolz>=0.9.0, which is not installed.\n",
            "distrax 0.1.2 requires jax>=0.1.55, which is not installed.\n",
            "jaxpruner 0.1 requires jax, which is not installed.\n",
            "ml-collections 0.1.1 requires PyYAML, which is not installed.\n",
            "optax 0.1.7 requires jax>=0.1.55, which is not installed.\n",
            "orbax 0.1.7 requires jax>=0.4.6, which is not installed.\n",
            "orbax 0.1.7 requires pyyaml, which is not installed.\n",
            "orbax-checkpoint 0.3.5 requires jax>=0.4.9, which is not installed.\n",
            "orbax-checkpoint 0.3.5 requires pyyaml, which is not installed.\n",
            "pdebench 0.1.0 requires scipy, which is not installed.\n",
            "qax 0.1.1 requires jax<0.5.0,>=0.4.10, which is not installed.\n",
            "tensorboard 2.12.3 requires markdown>=2.6.8, which is not installed.\n",
            "tensorflow-datasets 4.8.3 requires termcolor, which is not installed.\n",
            "tensorflow-datasets 4.8.3 requires wrapt, which is not installed.\n",
            "tensorflow-macos 2.12.0 requires astunparse>=1.6.0, which is not installed.\n",
            "tensorflow-macos 2.12.0 requires flatbuffers>=2.0, which is not installed.\n",
            "tensorflow-macos 2.12.0 requires gast<=0.4.0,>=0.2.1, which is not installed.\n",
            "tensorflow-macos 2.12.0 requires google-pasta>=0.1.1, which is not installed.\n",
            "tensorflow-macos 2.12.0 requires jax>=0.3.15, which is not installed.\n",
            "tensorflow-macos 2.12.0 requires opt-einsum>=2.3.2, which is not installed.\n",
            "tensorflow-macos 2.12.0 requires termcolor>=1.1.0, which is not installed.\n",
            "tensorflow-macos 2.12.0 requires wrapt<1.15,>=1.11.0, which is not installed.\n",
            "tensorflow-probability 0.19.0 requires cloudpickle>=1.3, which is not installed.\n",
            "tensorflow-probability 0.19.0 requires gast>=0.3.2, which is not installed.\n",
            "tf2jax 0.3.5 requires jax>=0.3.14, which is not installed.\n",
            "treex 0.6.10 requires PyYAML<7.0,>=6.0, which is not installed.\n",
            "zodiax 0.4.1 requires jax, which is not installed.\n",
            "haliax 1.0.1 requires equinox~=0.9.0, but you have equinox 0.10.9 which is incompatible.\n",
            "tensorflow-macos 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.2 which is incompatible.\n",
            "tensorflow-metadata 1.12.0 requires protobuf<4,>=3.13, but you have protobuf 4.24.2 which is incompatible.\n",
            "treex 0.6.10 requires flax<0.5.0,>=0.4.0, but you have flax 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.82 requires toolz>=0.9.0, which is not installed.\n",
            "flax 0.7.0 requires PyYAML>=5.4.1, which is not installed.\n",
            "orbax 0.1.7 requires pyyaml, which is not installed.\n",
            "orbax-checkpoint 0.3.5 requires pyyaml, which is not installed.\n",
            "tensorflow-macos 2.12.0 requires astunparse>=1.6.0, which is not installed.\n",
            "tensorflow-macos 2.12.0 requires flatbuffers>=2.0, which is not installed.\n",
            "tensorflow-macos 2.12.0 requires gast<=0.4.0,>=0.2.1, which is not installed.\n",
            "tensorflow-macos 2.12.0 requires google-pasta>=0.1.1, which is not installed.\n",
            "tensorflow-macos 2.12.0 requires termcolor>=1.1.0, which is not installed.\n",
            "tensorflow-macos 2.12.0 requires wrapt<1.15,>=1.11.0, which is not installed.\n",
            "treex 0.6.10 requires PyYAML<7.0,>=6.0, which is not installed.\n",
            "haliax 1.0.1 requires equinox~=0.9.0, but you have equinox 0.10.9 which is incompatible.\n",
            "tensorflow-macos 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.2 which is incompatible.\n",
            "treex 0.6.10 requires flax<0.5.0,>=0.4.0, but you have flax 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "treex 0.6.10 requires PyYAML<7.0,>=6.0, which is not installed.\n",
            "haliax 1.0.1 requires equinox~=0.9.0, but you have equinox 0.10.9 which is incompatible.\n",
            "treex 0.6.10 requires flax<0.5.0,>=0.4.0, but you have flax 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install keras-core --quiet\n",
        "!pip install git+https://github.com/ASEM000/serket --quiet\n",
        "!pip install optax --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXc4hBALZkGD",
        "outputId": "e95d6394-93b5-4d56-8136-505bce30d9b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using JAX backend.\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "This version of jax requires jaxlib version >= 0.4.11.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[0;32m~/miniforge3/envs/dev-jax/lib/python3.10/site-packages/jax/_src/lib/__init__.py:34\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m   \u001b[39mimport\u001b[39;00m \u001b[39mjaxlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m     36\u001b[0m   \u001b[39m# jaxlib is too old to have version number.\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jaxlib.version'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m/Users/asem/serket/docs/notebooks/train_mnist.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/asem/serket/docs/notebooks/train_mnist.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/asem/serket/docs/notebooks/train_mnist.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mKERAS_BACKEND\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mjax\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/asem/serket/docs/notebooks/train_mnist.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m mnist  \u001b[39m# for mnist only\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/asem/serket/docs/notebooks/train_mnist.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjax\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/asem/serket/docs/notebooks/train_mnist.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mjnp\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/dev-jax/lib/python3.10/site-packages/keras_core/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m \u001b[39mimport\u001b[39;00m activations\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m \u001b[39mimport\u001b[39;00m applications\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n",
            "File \u001b[0;32m~/miniforge3/envs/dev-jax/lib/python3.10/site-packages/keras_core/activations/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mactivations\u001b[39;00m \u001b[39mimport\u001b[39;00m deserialize\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mactivations\u001b[39;00m \u001b[39mimport\u001b[39;00m get\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mactivations\u001b[39;00m \u001b[39mimport\u001b[39;00m serialize\n",
            "File \u001b[0;32m~/miniforge3/envs/dev-jax/lib/python3.10/site-packages/keras_core/src/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m activations\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m applications\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n",
            "File \u001b[0;32m~/miniforge3/envs/dev-jax/lib/python3.10/site-packages/keras_core/src/activations/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtypes\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mactivations\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mactivations\u001b[39;00m \u001b[39mimport\u001b[39;00m elu\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mactivations\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mactivations\u001b[39;00m \u001b[39mimport\u001b[39;00m exponential\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mactivations\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mactivations\u001b[39;00m \u001b[39mimport\u001b[39;00m gelu\n",
            "File \u001b[0;32m~/miniforge3/envs/dev-jax/lib/python3.10/site-packages/keras_core/src/activations/activations.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi_export\u001b[39;00m \u001b[39mimport\u001b[39;00m keras_core_export\n",
            "File \u001b[0;32m~/miniforge3/envs/dev-jax/lib/python3.10/site-packages/keras_core/src/backend/__init__.py:36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39melif\u001b[39;00m backend() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mjax\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     35\u001b[0m     print_msg(\u001b[39m\"\u001b[39m\u001b[39mUsing JAX backend.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjax\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39melif\u001b[39;00m backend() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     38\u001b[0m     print_msg(\u001b[39m\"\u001b[39m\u001b[39mUsing PyTorch backend.\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniforge3/envs/dev-jax/lib/python3.10/site-packages/keras_core/src/backend/jax/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjax\u001b[39;00m \u001b[39mimport\u001b[39;00m core\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjax\u001b[39;00m \u001b[39mimport\u001b[39;00m image\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjax\u001b[39;00m \u001b[39mimport\u001b[39;00m math\n",
            "File \u001b[0;32m~/miniforge3/envs/dev-jax/lib/python3.10/site-packages/keras_core/src/backend/jax/core.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjax\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mjnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/dev-jax/lib/python3.10/site-packages/jax/__init__.py:35\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdel\u001b[39;00m _cloud_tpu_init\n\u001b[1;32m     32\u001b[0m \u001b[39m# Confusingly there are two things named \"config\": the module and the class.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# We want the exported object to be the class, so we first import the module\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# to make sure a later import doesn't overwrite the class.\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m \u001b[39mimport\u001b[39;00m config \u001b[39mas\u001b[39;00m _config_module\n\u001b[1;32m     36\u001b[0m \u001b[39mdel\u001b[39;00m _config_module\n\u001b[1;32m     38\u001b[0m \u001b[39m# Force early import, allowing use of `jax.core` after importing `jax`.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/dev-jax/lib/python3.10/site-packages/jax/config.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2018 The JAX Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[39m# TODO(phawkins): fix users of this alias and delete this file.\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_src\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m config\n",
            "File \u001b[0;32m~/miniforge3/envs/dev-jax/lib/python3.10/site-packages/jax/_src/config.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mthreading\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Callable, Generic, NamedTuple, Optional, TypeVar\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_src\u001b[39;00m \u001b[39mimport\u001b[39;00m lib\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_src\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m \u001b[39mimport\u001b[39;00m jax_jit\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_src\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m \u001b[39mimport\u001b[39;00m transfer_guard_lib\n",
            "File \u001b[0;32m~/miniforge3/envs/dev-jax/lib/python3.10/site-packages/jax/_src/lib/__init__.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m     36\u001b[0m   \u001b[39m# jaxlib is too old to have version number.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m   msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThis version of jax requires jaxlib version >= \u001b[39m\u001b[39m{\u001b[39;00m_minimum_jaxlib_version_str\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 38\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# Checks the jaxlib version before importing anything else from jaxlib.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# Returns the jaxlib version string.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_jaxlib_version\u001b[39m(jax_version: \u001b[39mstr\u001b[39m, jaxlib_version: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     44\u001b[0m                          minimum_jaxlib_version: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]:\n\u001b[1;32m     45\u001b[0m   \u001b[39m# Regex to match a dotted version prefix 0.1.23.456.789 of a PEP440 version.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m   \u001b[39m# PEP440 allows a number of non-numeric suffixes, which we allow also.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m   \u001b[39m# We currently do not allow an epoch.\u001b[39;00m\n",
            "\u001b[0;31mImportError\u001b[0m: This version of jax requires jaxlib version >= 0.4.11."
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "from keras_core.datasets import mnist  # for mnist only\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import functools as ft\n",
        "import optax  # for gradient optimization\n",
        "import serket as sk\n",
        "import time\n",
        "import matplotlib.pyplot as plt  # for plotting the predictions\n",
        "\n",
        "EPOCHS = 1\n",
        "LR = 1e-3\n",
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W8_JuFbZkGF"
      },
      "source": [
        "#### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foBqrS8VZkGF"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), _ = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(-1, 1, 28, 28).astype(\"float32\") / 255.0\n",
        "x_train = jnp.array_split(x_train, x_train.shape[0] // BATCH_SIZE)\n",
        "y_train = jnp.array_split(y_train, y_train.shape[0] // BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2zMI_8OZkGG"
      },
      "source": [
        "#### Model creation\n",
        "\n",
        "_**Style 1**_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZxrY-foZkGG"
      },
      "outputs": [],
      "source": [
        "k1, k2, k3 = jax.random.split(jax.random.PRNGKey(0), 3)\n",
        "\n",
        "nn = sk.nn.Sequential(\n",
        "    sk.nn.Conv2D(1, 32, 3, key=k1, padding=\"valid\"),\n",
        "    jax.nn.relu,\n",
        "    sk.nn.MaxPool2D(2, 2),\n",
        "    sk.nn.Conv2D(32, 64, 3, key=k2, padding=\"valid\"),\n",
        "    jax.nn.relu,\n",
        "    sk.nn.MaxPool2D(2, 2),\n",
        "    jnp.ravel,\n",
        "    sk.nn.Linear(1600, 10, key=k3),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx9tylogZkGH"
      },
      "source": [
        "_**Style 2**_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEB4UzU7ZkGH"
      },
      "outputs": [],
      "source": [
        "k1, k2, k3 = jax.random.split(jax.random.PRNGKey(0), 3)\n",
        "\n",
        "\n",
        "@sk.autoinit\n",
        "class ConvNet(sk.TreeClass):\n",
        "    def __init__(self):\n",
        "        self.conv1 = sk.nn.Conv2D(1, 32, 3, key=k1, padding=\"valid\")\n",
        "        self.pool1 = sk.nn.MaxPool2D(2, 2)\n",
        "        self.conv2 = sk.nn.Conv2D(32, 64, 3, key=k2, padding=\"valid\")\n",
        "        self.pool2 = sk.nn.MaxPool2D(2, 2)\n",
        "        self.linear = sk.nn.Linear(1600, 10, key=k3)\n",
        "\n",
        "    def __call__(self, x: jax.Array) -> jax.Array:\n",
        "        x = self.pool1(jax.nn.relu(self.conv1(x)))\n",
        "        x = self.pool2(jax.nn.relu(self.conv2(x)))\n",
        "        x = self.linear(jnp.ravel(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "nn = ConvNet()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s0KCnVEZkGI"
      },
      "source": [
        "#### Visualize tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kZMEgaYZkGI",
        "outputId": "421dbebf-2a53-45f5-95d4-b6926c1ed5a8"
      },
      "outputs": [],
      "source": [
        "print(\"depth=0\")\n",
        "print(sk.tree_summary(nn, depth=0))\n",
        "print(\"depth=1\")\n",
        "print(sk.tree_summary(nn, depth=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Apr03C_ZkGJ"
      },
      "source": [
        "#### Training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9X_xMaWEZkGJ"
      },
      "outputs": [],
      "source": [
        "# 1) mask the non-jaxtype parameters\n",
        "nn = sk.tree_mask(nn)\n",
        "\n",
        "# 2) initialize the optimizer state\n",
        "optim = optax.adam(LR)\n",
        "optim_state = optim.init(nn)\n",
        "\n",
        "\n",
        "@jax.vmap\n",
        "def softmax_cross_entropy(logits, onehot):\n",
        "    assert onehot.shape == logits.shape == (10,)\n",
        "    return -jnp.sum(jax.nn.log_softmax(logits) * onehot)\n",
        "\n",
        "\n",
        "@ft.partial(jax.grad, has_aux=True)\n",
        "def loss_func(nn, x, y):\n",
        "    # 3) unmask the non-jaxtype parameters to be used in the computation.\n",
        "    # The tree must be masked -using sk.tree_mask` before being passed to the\n",
        "    # loss function. Because `jax.grad` will not be able to differentiate through\n",
        "    # the tree if the tree contains non-jaxtype parameters (e.g. anything that is not\n",
        "    # a float/float array/complex/complex array like activation function or strings).\n",
        "    # So we mask before passing to the loss function, and unmask before calling the\n",
        "    # neural network, thus rendering the non-jaxtype parameters *invisible* to `jax.grad`\n",
        "    nn = sk.tree_unmask(nn)\n",
        "\n",
        "    # 4) vectorize the computation over the batch\n",
        "    logits = jax.vmap(nn)(x)\n",
        "    onehot = jax.nn.one_hot(y, 10)\n",
        "\n",
        "    # 5) use the appropriate loss function\n",
        "    loss = jnp.mean(softmax_cross_entropy(logits, onehot))\n",
        "    return loss, (loss, logits)\n",
        "\n",
        "\n",
        "@jax.vmap\n",
        "def accuracy_func(logits, y):\n",
        "    assert logits.shape == (10,)\n",
        "    return jnp.argmax(logits) == y\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def train_step(nn, optim_state, x, y):\n",
        "    # the loss function will have an output of (loss, logits)\n",
        "    # as an auxillary output, and will have a gradient `grads` of same\n",
        "    # structure as the input\n",
        "    grads, (loss, logits) = loss_func(nn, x, y)\n",
        "\n",
        "    # 6) update the parameters using the optimizer\n",
        "    updates, optim_state = optim.update(grads, optim_state)\n",
        "    nn = optax.apply_updates(nn, updates)\n",
        "    return nn, optim_state, (loss, logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBHo1WsKZkGJ"
      },
      "source": [
        "#### Train and plot results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "9Qr1nuU8ZkGJ",
        "outputId": "2449b463-3003-4a15-b101-68909dd5598f"
      },
      "outputs": [],
      "source": [
        "for i in range(1, EPOCHS + 1):\n",
        "    t0 = time.time()\n",
        "    for j, (xb, yb) in enumerate(zip(x_train, y_train)):\n",
        "        nn, optim_state, (loss, logits) = train_step(nn, optim_state, xb, yb)\n",
        "        accuracy = jnp.mean(accuracy_func(logits, yb))\n",
        "        print(\n",
        "            f\"Epoch: {i:003d}/{EPOCHS:003d}\\t\"\n",
        "            f\"Batch: {j+1:003d}/{len(x_train):003d}\\t\"\n",
        "            f\"Batch loss: {loss:3e}\\t\"\n",
        "            f\"Batch accuracy: {accuracy:3f}\\t\"\n",
        "            f\"Time: {time.time() - t0:.3f}\",\n",
        "            end=\"\\r\",\n",
        "        )\n",
        "\n",
        "# 6) un-mask the trained network\n",
        "nn = sk.tree_unmask(nn)\n",
        "\n",
        "# create 2x5 grid of images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
        "idxs = jax.random.randint(k1, shape=(10,), minval=0, maxval=x_train[0].shape[0])\n",
        "\n",
        "for i, idx in zip(axes.flatten(), idxs):\n",
        "    # get the prediction\n",
        "    pred = nn(x_train[0][idx])\n",
        "    # plot the image\n",
        "    i.imshow(x_train[0][idx].reshape(28, 28), cmap=\"gray\")\n",
        "    # set the title to be the prediction\n",
        "    i.set_title(jnp.argmax(pred))\n",
        "    i.set_xticks([])\n",
        "    i.set_yticks([])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dev-jax",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
